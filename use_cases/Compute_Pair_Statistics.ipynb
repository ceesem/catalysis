{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Homologous Pair Statistics\n",
    "This notebook uses the defined neuron and lineage homologies from annotations and the `BrainPairCensus` annotation to measure three kinds of properties for matched neurons.\n",
    "    1) Lineage assignment and soma-soma distance\n",
    "    2) NBLAST score\n",
    "    3) Connectivity to identified homologous neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# SET THE PATHS APPROPRIATELY\n",
    "sys.path.append('path_to_catpy')\n",
    "sys.path.append('path_to_catalysis')\n",
    "epsilon = sys.float_info.epsilon\n",
    "\n",
    "import catalysis as cat\n",
    "import catalysis.pynblast as pynblast\n",
    "import catalysis.plt as catplt\n",
    "import catalysis.transform as transform\n",
    "import catalysis.completeness as completeness\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as cl\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "from itertools import chain\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this with the correct information following the template in data/example_project_info.json.\n",
    "my_json_project_file =  'my_json_project_file.json'\n",
    "l1data = cat.CatmaidDataInterface.from_json( my_json_project_file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pulls all lineage annotations, in this case based on particular string matches. This could be replaced down the road with, e.g., meta-annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_match = re.compile('\\*.*akira')\n",
    "lineage_parser = re.compile('\\*(?P<group>.*?)_(?P<instance>[rl]) akira')\n",
    "hemilateral_groups = l1data.match_groups_from_select_annotations( wb_match, lineage_parser )\n",
    "lineages = sorted(list(hemilateral_groups.keys()))\n",
    "\n",
    "# Let's shove the sensory neurons in here too.\n",
    "hemilateral_groups['sensory'] = {'l':'Brain&SEZ sensory left akira','r':'Brain&SEZ sensory right akira'}\n",
    "lineages.append('sensory')\n",
    "\n",
    "left_names = [hemilateral_groups[lin]['l'] for lin in hemilateral_groups if len(hemilateral_groups[lin])>1]\n",
    "right_names = [hemilateral_groups[lin]['r'] for lin in hemilateral_groups if len(hemilateral_groups[lin])>1 ]\n",
    "\n",
    "left_anno_ids = l1data.get_annotation_ids_from_names(left_names)\n",
    "right_anno_ids = l1data.get_annotation_ids_from_names(right_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_skids = l1data.get_ids_from_annotations(left_anno_ids,flatten=True)\n",
    "right_skids = l1data.get_ids_from_annotations(right_anno_ids,flatten=True)\n",
    "\n",
    "id_pairs_matched = []\n",
    "id_pairs_ipsi = []\n",
    "id_pairs_contra = []\n",
    "\n",
    "for lin in sorted(hemilateral_groups):\n",
    "    print(lin)\n",
    "    if len(hemilateral_groups[lin]) > 1:\n",
    "        if lin == 'sensory':\n",
    "            ipm, ipi, ipc = completeness.make_id_pairs(l1data,\n",
    "                hemilateral_groups['sensory']['l'],\n",
    "                hemilateral_groups['sensory']['r'],\n",
    "                'BrainPairCensus',\n",
    "                max_open_ends=0.05,\n",
    "                min_node_count=300,\n",
    "                sensory_exception=True)\n",
    "        else:\n",
    "            ipm, ipi, ipc = completeness.make_id_pairs(l1data,\n",
    "                            hemilateral_groups[lin]['l'],\n",
    "                            hemilateral_groups[lin]['r'],\n",
    "                            'BrainPairCensus')\n",
    "        id_pairs_matched.append(ipm)\n",
    "        id_pairs_ipsi.append(ipi)\n",
    "        id_pairs_contra.append(ipc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of id pairs on left and right that are from homologous neurons.\n",
    "id_pairs_matched_l = [pair for pair in chain.from_iterable(id_pairs_matched)]\n",
    "\n",
    "# A list of ids pairs within the same side that are in the same lineage, but not homologous neurons.\n",
    "id_pairs_ipsi_l = [pair for pair in chain.from_iterable(id_pairs_ipsi)]\n",
    "\n",
    "# A list of id pairs on left and right that are the homologous lineage, but not the homologous neuron.\n",
    "id_pairs_contra_l = [pair for pair in chain.from_iterable(id_pairs_contra)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all matched pairs if you're happy wiht the analysis, since as you've seen it takes a while to compute.\n",
    "##### (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_pairs_all', 'wb') as fid:\n",
    "    pickle.dump({'matched': id_pairs_matched_l, 'ipsi':id_pairs_ipsi_l, 'contra': id_pairs_contra_l}, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Measure properties of matched neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Compare the anatomical similarity of matched neurons and unmatched neurons after landmark-based transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import an NBLAST distance mat your favorite way\n",
    "adult_fly_f = pd.read_csv(\"catalysis/data/smat_jefferis.csv\",delimiter=' ')\n",
    "smat = pynblast.ScoreMatrixLookup.from_dataframe( adult_fly_f )\n",
    "# Do this to reduce things to L1 volume size and change scale to nm, based on scale up observed in the L1/L3 data papers and desired properties\n",
    "smat.d_range = smat.d_range * 1000 / 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ids = [skid for skid in chain.from_iterable(id_pairs_matched_l)]\n",
    "pair_nrns = cat.NeuronList.from_id_list(id_list=pair_ids,CatmaidInterface=l1data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For left/right homologous pair, compute the NBLAST similarity and the distance between somata after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_match = dict()\n",
    "Dsoma_match = dict()\n",
    "for pair in tqdm.tqdm(id_pairs_matched_l):\n",
    "    nrn_q = pair_nrns[pair[0]]\n",
    "    nrn_t = transform.transform_neuron_from_landmarks(pair_nrns[pair[1]],\n",
    "                                    from_group='brain hemisphere right',\n",
    "                                    to_group='brain hemisphere left',\n",
    "                                    CatmaidInterface=l1data,\n",
    "                                    contralateral=True )\n",
    "    S_match[str(pair)] = pynblast.exact_nblast(smat, [nrn_q], [nrn_t] ).values[0][0]\n",
    "\n",
    "    if not completeness.fragment_test(nrn_q) and not completeness.fragment_test(nrn_t):\n",
    "        xyz_q = np.array(nrn_q.nodeloc[ nrn_q.tags['soma'][0] ])\n",
    "        xyz_t = np.array(nrn_t.nodeloc[ nrn_t.tags['soma'][0] ])\n",
    "        Dsoma_match[str(pair)] = np.linalg.norm(xyz_q-xyz_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For every left/right same-lineage, but non-homologous pair, compute the NBLAST similarity and the distance between somata after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_contra = list(set([skid for skid in chain.from_iterable(id_pairs_contra_l)]))\n",
    "print(len(ids_contra))\n",
    "nrns_contra = cat.NeuronList.from_id_list(id_list=ids_contra,CatmaidInterface=l1data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_contra = dict()\n",
    "Dsoma_contra = dict()\n",
    "\n",
    "vals = np.random.choice(len(id_pairs_contra_l), 300 )\n",
    "\n",
    "for val in tqdm.tqdm(vals):\n",
    "    pair = id_pairs_contra_l[val]\n",
    "    nrn_q = nrns_contra[pair[0]]\n",
    "    nrn_t = transform.transform_neuron_from_landmarks( nrns_contra[pair[1]],\n",
    "                                    from_group='brain hemisphere right',\n",
    "                                    to_group='brain hemisphere left',\n",
    "                                    CatmaidInterface=l1data,\n",
    "                                    contralateral=True )\n",
    "\n",
    "    S_contra[str(pair)] = pynblast.exact_nblast(smat, [nrn_q], [nrn_t] ).values[0][0]\n",
    "\n",
    "    if not completeness.fragment_test(nrn_q) and not completeness.fragment_test(nrn_t):\n",
    "        xyz_q = np.array(nrn_q.nodeloc[ nrn_q.tags['soma'][0] ])\n",
    "        xyz_t = np.array(nrn_t.nodeloc[ nrn_t.tags['soma'][0] ])\n",
    "        Dsoma_contra[str(pair)] = np.linalg.norm(xyz_q-xyz_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the soma distance and similarity scores as a check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(np.array([d for d in Dsoma_match.values()])/1000,cumulative=True,cut=0)\n",
    "sns.kdeplot(np.array([d for d in Dsoma_contra.values()])/1000,cumulative=True,cut=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(np.array([d for d in S_match.values()]),cumulative=True,cut=0)\n",
    "sns.kdeplot(np.array([d for d in S_contra.values()]),cumulative=True,cut=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Get p(matched | S, d_soma ), p( unmatched | S, d_soma ), p( S ), and p( d_soma )\n",
    "In order to get the basic statistics, we need to generate three different lists of id pairs of *complete* neurons only.\n",
    "\n",
    "1. Matched partners.\n",
    "2. Same lineage, same side.\n",
    "3. Same lineage, opposite side but unmatched.\n",
    "\n",
    "Because the data is somewhat limited (and basically doesn't scale to values of similarity that are indeed possible to achieve), we're going to take a model based approached.\n",
    "\n",
    "##### First, fit likelihood based on NBLAST similarity with logistic function.\n",
    "Note: The drop-off in match probability for very high values of similarity is clearly non-realistic, but just means that we don't usually see such high values in our data. Nonetheless, let's use a logistic function approximation to be robust to the chance we do see higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sdens_match = sp.stats.gaussian_kde([d for d in S_match.values()])\n",
    "Sdens_contra = sp.stats.gaussian_kde([d for d in S_contra.values()])\n",
    "#Sdens_ipsi = sp.stats.gaussian_kde([d for d in S_ipsi.values()])\n",
    "\n",
    "# Note that the choice of fit functions is explicitly connected to pynblast._logistic_ratio()\n",
    "fitfun = lambda p, x : p[0] / ( 1 + np.exp( -1 * p[1] * (x - p[2]) ) )\n",
    "errfun = lambda p, x, y: fitfun(p,x) - y\n",
    "\n",
    "xs = np.arange(0,0.8, 0.01)\n",
    "pair_vals = Sdens_match.evaluate(xs)\n",
    "tot_vals_contra = Sdens_match.evaluate(xs)+Sdens_contra.evaluate(xs)\n",
    "dat = pair_vals/tot_vals_contra\n",
    "\n",
    "p0 = [0.9, 1, 0.6]\n",
    "shape_fit_param, success = sp.optimize.leastsq( errfun, p0, args=(xs, dat) )\n",
    "\n",
    "## Check the fit quality\n",
    "\n",
    "xs = np.arange(0,1,0.02)\n",
    "pair_vals = Sdens_match.evaluate(xs)\n",
    "tot_vals_contra = Sdens_match.evaluate(xs)+Sdens_contra.evaluate(xs)\n",
    "non_match_contra = Sdens_contra.evaluate(xs)\n",
    "\n",
    "plt.plot(xs,pair_vals / tot_vals_contra)\n",
    "plt.plot(xs, fitfun(shape_fit_param,xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the soma distances\n",
    "\n",
    "Here, our model is a bit more complicated. The matched and non-matched soma distance distribution can be well-fit with gamma distributions.\n",
    "However, if that's all we do, then extremely close soma distances are treated as a bit stronger evidence than we observe in the data (it's highly suggestive, but will never prove anything).\n",
    "We thus add a short-range expontential weakening of the likelihood function to give a better heuristic fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ddens_match = sp.stats.gaussian_kde([d for d in Dsoma_match.values()])\n",
    "Ddens_contra = sp.stats.gaussian_kde([d for d in Dsoma_contra.values()])\n",
    "\n",
    "xs = np.arange(0,46000,1000)\n",
    "pair_vals = Ddens_match.evaluate(xs)\n",
    "Dtot_vals_contra = Ddens_match.evaluate(xs)+Ddens_contra.evaluate(xs)\n",
    "\n",
    "match_param = sp.stats.gamma.fit(list(Dsoma_match.values()), floc=0 )\n",
    "contra_param = sp.stats.gamma.fit(list(Dsoma_contra.values()), floc=0 )\n",
    "\n",
    "xval = np.arange(0,30000,1000)\n",
    "yval = sp.stats.gamma.pdf(xval, *match_param )\n",
    "sns.distplot(list(Dsoma_match.values()))\n",
    "plt.plot(xval,yval)\n",
    "\n",
    "xvalc = np.arange(0,30000,1000)\n",
    "yvalc = sp.stats.gamma.pdf(xvalc, *contra_param )\n",
    "sns.distplot(list(Dsoma_contra.values()))\n",
    "plt.plot(xvalc,yvalc)\n",
    "plt.xlabel('Soma Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, note that the following is explicitly tied to pynblast._gamma_ratio()\n",
    "def gamma_odds( xs, num_param, denom_param, exp_param ):\n",
    "    num = sp.stats.gamma.pdf( xs, *num_param )\n",
    "    denom = sp.stats.gamma.pdf( xs, *denom_param)\n",
    "    return num / (num+denom) * ( 1-exp_param[0] * np.exp(-xs/exp_param[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitfun_gr = lambda p, x : gamma_odds( x[0], x[1], x[2],  p)\n",
    "errfun_gr = lambda p, x, y: fitfun_gr(p,x) - y\n",
    "\n",
    "xs = np.arange(1,40000,100)\n",
    "pair_vals = Ddens_match.evaluate(xs)\n",
    "Dtot_vals_contra = Ddens_match.evaluate(xs)+Ddens_contra.evaluate(xs)\n",
    "dat = pair_vals / Dtot_vals_contra\n",
    "\n",
    "p_gr0 = (0.2, 2000)\n",
    "exp_param, success = sp.optimize.leastsq( errfun_gr, p_gr0, args=( (xs,match_param,contra_param) , dat ) )\n",
    "\n",
    "plt.plot(xs, pair_vals / Dtot_vals_contra)\n",
    "plt.plot(xs, gamma_odds( xs, match_param, contra_param, exp_param ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the fits that we just did for future comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('morpho_stats','wb') as fid:\n",
    "    pickle.dump({'shape':shape_fit_param,\n",
    "                 'dist_match':match_param,\n",
    "                 'dist_nonmatch':contra_param,\n",
    "                 'exp_cutoff':exp_param}, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Look at connection weights between paired neurons\n",
    "\n",
    "For every complete neuron in the pair list, find its complete synaptic partners.\n",
    "For every complete partner that is on the pair list, find the matched connection as well.\n",
    "We will do this separately for upstream and downstream.\n",
    "\n",
    "But also, compute P(n_syn) for all connections, so we can compare P(n_syn_1,n_syn_2) to P(n_syn_1)P(n_syn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the homologous neuron id pairs based on the meta-annotation `BrainPairCensus` that have been completed to a satisfactory degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_annos = [hemilateral_groups[lin]['l'] for lin in hemilateral_groups if len(hemilateral_groups[lin])>1]\n",
    "right_annos = [hemilateral_groups[lin]['r'] for lin in hemilateral_groups if len(hemilateral_groups[lin])>1]\n",
    "\n",
    "all_left_ids = l1data.get_ids_from_annotations(left_annos, flatten=True)\n",
    "all_right_ids = l1data.get_ids_from_annotations(right_annos, flatten=True)\n",
    "\n",
    "pair_meta = 'BrainPairCensus'\n",
    "pair_annos = l1data.get_annotations_from_meta_annotations( pair_meta, flatten=True)\n",
    "paired_ids = pair_nrns.ids()\n",
    "\n",
    "ids_paired_left = list( set(all_left_ids).intersection(paired_ids) )\n",
    "ids_paired_right = list( set(all_right_ids).intersection(paired_ids) )\n",
    "\n",
    "reload(completeness)\n",
    "match_report = completeness.match_report( ids_paired_left, ids_paired_right, match_via=pair_meta, CatmaidInterface=l1data, name1='Left', name2='Right', anno_reference = 'names', skip_annos=None, show_completeness=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the adjacency matrix, get a list of paired homologous neuron->neuron edges only of completed neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, skid_to_ind, ind_to_skid = pair_nrns.get_adjacency_matrix()\n",
    "\n",
    "new_order = np.zeros((len(A)),dtype=np.int64)-1\n",
    "curr_ind = 0\n",
    "mirror_ind_start = len(ids_paired_left)\n",
    "rel_skids = []\n",
    "\n",
    "for ind, row in enumerate(match_report.iterrows()):\n",
    "    if np.size(row[1]['Left']) > 0:\n",
    "        new_order[curr_ind] = skid_to_ind[row[1]['Left']]\n",
    "        new_order[mirror_ind_start+curr_ind] = skid_to_ind[row[1]['Right']]\n",
    "        curr_ind += 1\n",
    "new_order = new_order[np.where(new_order>=0)]\n",
    "\n",
    "Anew = A[:,new_order][new_order]\n",
    "\n",
    "norm_inds = np.arange(0,len(Anew))\n",
    "mirror_inds = np.mod(np.arange(mirror_ind_start,mirror_ind_start+len(Anew)),len(Anew))\n",
    "\n",
    "pair_mirror_dict = {'skid_to_ind':dict()}\n",
    "pair_mirror_dict['skid_to_ind'] = {ind_to_skid[ind]:ii for ii,ind in enumerate(new_order)}\n",
    "pair_mirror_dict['mirror_inds'] = mirror_inds\n",
    "\n",
    "new_order = np.zeros((len(A)),dtype=np.int64)-1\n",
    "curr_ind = 0\n",
    "mirror_ind_start = len(ids_paired_left)\n",
    "rel_skids = []\n",
    "\n",
    "for aid in match_report:\n",
    "    for ind, nid in enumerate(match_report[aid][0]):\n",
    "        if ind < len(match_report[aid][1]):\n",
    "            new_order[curr_ind] = skid_to_ind[nid]\n",
    "            new_order[mirror_ind_start+curr_ind] = skid_to_ind[match_report[aid][1][ind]]\n",
    "            curr_ind += 1\n",
    "new_order = new_order[np.where(new_order>=0)]\n",
    "\n",
    "Anew = A[:,new_order][new_order]\n",
    "\n",
    "norm_inds = np.arange(0,len(Anew))\n",
    "mirror_inds = np.mod(np.arange(mirror_ind_start,mirror_ind_start+len(Anew)),len(Anew))\n",
    "\n",
    "pair_mirror_dict = {'skid_to_ind':dict()}\n",
    "pair_mirror_dict['skid_to_ind'] = {ind_to_skid[ind]:ii for ii,ind in enumerate(new_order)}\n",
    "pair_mirror_dict['mirror_inds'] = mirror_inds\n",
    "\n",
    "pairs = []\n",
    "for ind1 in np.arange(0,mirror_ind_start):\n",
    "    for ind2 in norm_inds:\n",
    "        w_norm = Anew[ind2,ind1]\n",
    "        w_mirror = Anew[mirror_inds[ind2],mirror_inds[ind1]]\n",
    "#         if w_norm > 0 and w_mirror > 0:\n",
    "        pairs.append([ w_norm, w_mirror] )\n",
    "pairs=np.array(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the frequency of synapse counts both absolutely, and correlated between homologous edges. This lets us compute the likelihood ratio that a given pair of connections comes from matched versus independent draws, at least in a naive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amatch_2d = sp.stats.gaussian_kde(\n",
    "    np.concatenate( ( np.vstack((pairs[:,1],pairs[:,0])),np.vstack((pairs[:,0],pairs[:,1]))),axis=1),\n",
    "    bw_method=4)\n",
    "Afreq = sp.stats.gaussian_kde( pairs.reshape(1,pairs.size), bw_method=4)\n",
    "\n",
    "max_syn = 100\n",
    "\n",
    "norm_1d = 0\n",
    "for n in np.arange(max_syn):\n",
    "    norm_1d += Afreq(n)\n",
    "\n",
    "Afreq_mat = np.zeros( (max_syn) )\n",
    "for n in np.arange(max_syn):\n",
    "    Afreq_mat[n] = Afreq(n)\n",
    "\n",
    "Amatch_ratio_mat = np.zeros( (max_syn, max_syn) )\n",
    "num_norm = 0\n",
    "for n in np.arange(max_syn):\n",
    "    for m in np.arange(max_syn):\n",
    "        num_norm += Amatch_2d((n,m))\n",
    "\n",
    "for n in np.arange(max_syn):\n",
    "    for m in np.arange(max_syn):\n",
    "        Amatch_ratio_mat[n,m] = np.log2( Amatch_2d((n,m))/num_norm + epsilon ) - np.log2( Afreq_mat[n]/norm_1d * Afreq_mat[m]/norm_1d + epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Amatch_ratio_mat heatmap as a sanity check. You'd expect x~y to be the hottest areas, since that indicates similar connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(Amatch_ratio_mat[0:60,0:60], cmap=\"RdBu\", center=0, vmin=-5, vmax=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the connectivity fits and pair map we generated for use in the homology-checking completeness workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('matched_synapse_prob','wb') as fid:\n",
    "    dill.dump({'log_ratio': Amatch_ratio_mat, 'match':Amatch_2d, 'freq':Afreq},fid)\n",
    "\n",
    "with open('pair_maps', 'wb') as fid:\n",
    "    dill.dump({'pair_maps':pair_mirror_dict}, fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nrn",
   "language": "python",
   "name": "nrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
